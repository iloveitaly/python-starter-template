"""base_models

Revision ID: 816d99f62e46
Revises: 489aff797e2e
Create Date: 2025-09-24 22:16:34.376545

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
import sqlmodel
import activemodel
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '816d99f62e46'
down_revision: Union[str, None] = '489aff797e2e'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('llm_response')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('llm_response',
    sa.Column('id', sa.UUID(), autoincrement=False, nullable=False, comment='TypeID with prefix: llr'),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('model', sa.VARCHAR(), autoincrement=False, nullable=False, comment='the AI model used to generate the response'),
    sa.Column('response', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('prompt', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('category', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('prompt_hash', sa.VARCHAR(), autoincrement=False, nullable=False, comment='sha of the hash for easily retrieving the exact same prompt'),
    sa.PrimaryKeyConstraint('id', name=op.f('llm_response_pkey')),
    comment='Model to cache LLM responses for caching and debugging'
    )
    # ### end Alembic commands ###
